{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Union, Optional\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCartographyGenerativeTask:\n",
    "    def __init__(self, \n",
    "                 model_id: str, \n",
    "                 tokenizer_id: str,\n",
    "                 rouge_scorer_object: Optional[rouge_scorer]=None\n",
    "                 ) -> None:\n",
    "        self.model_id = model_id or \"t5-base\"\n",
    "        self.tokenizer_id = tokenizer_id or \"t5-base\"\n",
    "        self.scorer = rouge_scorer_object or rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        self.on = \"epoch\"\n",
    "        self._is_fitted = False\n",
    "\n",
    "        # These fields will be assigned by the _load_data() method\n",
    "        self.input_col_name = None\n",
    "        self.output_col_name = None\n",
    "        self.hf_data = None\n",
    "        self.input_files = None\n",
    "        self.input_labels = None\n",
    "        \n",
    "        # These fields are assigned by the _train_model() method\n",
    "        self.output_weight_path = None\n",
    "\n",
    "\n",
    "    def _load_data(self, input_file_or_path, input_col_name, output_col_name):\n",
    "        if isinstance(input_file_or_path, str):\n",
    "            df = pd.read_csv(input_file_or_path)\n",
    "        elif isinstance(input_file_or_path, pd.DataFrame):\n",
    "            df = input_file_or_path\n",
    "        self.hf_data = DatasetDict({\"train\":Dataset.from_pandas(df)})\n",
    "        self.input_col_name = input_col_name\n",
    "        self.output_col_name = output_col_name\n",
    "        try:\n",
    "            self.input_files = df[self.input_col_name].values\n",
    "        except:\n",
    "            raise Exception(f\"The input_col_name {self.input_col_name} is not a valid column name.\")\n",
    "        \n",
    "        try:\n",
    "            self.input_labels = df[self.output_col_name].values\n",
    "        except:\n",
    "            raise Exception(f\"The output_col_name {self.output_col_name} is not a valid column name.\")\n",
    "\n",
    "\n",
    "    def _train_model(self, training_arguments):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n",
    "\n",
    "        def tokenize_function(sample):\n",
    "            model_inputs = tokenizer(sample[self.input_col_name], max_length=512, truncation=True, padding=True)\n",
    "            labels = tokenizer(sample[self.output_col_name], max_length=512, truncation=True, padding=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = self.hf_data.map(tokenize_function, batched=True)\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(self.model_id)\n",
    "\n",
    "        training_args = training_arguments or \\\n",
    "            Seq2SeqTrainingArguments(output_dir=\"./output_weights\", num_train_epochs=5, save_strategy=\"epoch\", logging_strategy=\"epoch\")\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self._is_fitted = True\n",
    "        self.output_weight_path = training_args.output_dir\n",
    "\n",
    "\n",
    "    def _get_average_confidence(self, model_weights_path, batch_size):\n",
    "        ckpt_return = {}\n",
    "        for ckpt_i in tqdm(os.listdir(model_weights_path)):\n",
    "            if ckpt_i not in [\"runs\", \".DS_Store\"]:\n",
    "                ckpt_return[ckpt_i] = {}\n",
    "                model_ckpt_i = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                try:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                except:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(self.tokenizer_id)\n",
    "\n",
    "                batches = [self.input_files[i:i+batch_size] for i in range(0, len(self.input_files), batch_size)]\n",
    "                perplexity_ckpt_i = []\n",
    "\n",
    "                for batch in batches:\n",
    "                    input_batch = tokenizer_ckpt_i(batch.tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                    outputs = model_ckpt_i.generate(**input_batch, max_new_tokens=200, return_dict_in_generate=True, output_scores=True)\n",
    "                    transition_scores = model_ckpt_i.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "                    perplexity_ckpt_i.extend(transition_scores.exp().mean(dim=1).numpy().tolist())\n",
    "                ckpt_return[ckpt_i] = perplexity_ckpt_i\n",
    "        return pd.DataFrame(ckpt_return).to_numpy()\n",
    "\n",
    "\n",
    "    def _get_variability(self, average_perplexity_across_epochs):\n",
    "        return np.std(average_perplexity_across_epochs, axis=1)\n",
    "\n",
    "\n",
    "    def _get_correctness(self, model_weights_path, batch_size):\n",
    "        ckpt_return = {}\n",
    "        for ckpt_i in tqdm(os.listdir(model_weights_path)):\n",
    "            if ckpt_i not in [\"runs\", \".DS_Store\"]:\n",
    "                ckpt_return[ckpt_i] = {}\n",
    "                model_ckpt_i = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                try:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                except:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "                batches_input = [self.input_files[i:i+batch_size] for i in range(0, len(self.input_files), batch_size)]\n",
    "                batches_output = [self.input_labels[i:i+batch_size] for i in range(0, len(self.input_labels), batch_size)]\n",
    "                rouge_l_ckpt_i = []\n",
    "\n",
    "                for batch_in, batch_out in zip(batches_input, batches_output):\n",
    "                    input_batch = tokenizer_ckpt_i(batch_in.tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                    outputs = model_ckpt_i.generate(**input_batch, max_new_tokens=200, return_dict_in_generate=True)\n",
    "                    batch_gen = tokenizer_ckpt_i.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "                    for batch_out_i, batch_gen_i in zip(batch_out, batch_gen):\n",
    "                        scores = self.scorer.score(batch_out_i, batch_gen_i)\n",
    "                        score = scores[\"rougeL\"].fmeasure\n",
    "                        rouge_l_ckpt_i.append(score)\n",
    "                ckpt_return[ckpt_i] = rouge_l_ckpt_i\n",
    "        return pd.DataFrame(ckpt_return).mean(axis=1).to_numpy()\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            input_data_or_path: Union[str, pd.DataFrame],\n",
    "            input_col_name: Optional[str]=None,\n",
    "            output_col_name: Optional[str]=None,\n",
    "            training_arguments: Optional[TrainingArguments]=None\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Load dataset and assign instance variables related to data.\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        self._load_data(input_file_or_path=input_data_or_path, \n",
    "                        input_col_name=input_col_name or \"input\", \n",
    "                        output_col_name=output_col_name or \"output\")\n",
    "        self._train_model(training_arguments=training_arguments)\n",
    "\n",
    "\n",
    "    def transform(self,\n",
    "                  input_data_or_path: Optional[Union[str, pd.DataFrame]]=None,\n",
    "                  input_col_name: Optional[str]=None,\n",
    "                  output_col_name: Optional[str]=None,\n",
    "                  model_weights_path: Optional[str]=None,\n",
    "                  batch_size: Optional[int]=None\n",
    "                  ):\n",
    "        \"\"\"\n",
    "        Using the trained model checkpoints, do the following steps:\n",
    "        1. Get Average Confidence Across self.on\n",
    "        2. Get Variability\n",
    "        3. Get Correctness\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            if input_data_or_path is None or model_weights_path is None:\n",
    "                raise Exception(\"\"\"One of input_data_or_path or model_weights_path has not been assigned.\n",
    "                                Since the model has not been fit, instance fields cannot be derived.\n",
    "                                Pass values for the above mentioned arguments.\"\"\")\n",
    "            else:\n",
    "                self._load_data(input_file_or_path=input_data_or_path,\n",
    "                                input_col_name=input_col_name or \"input\",\n",
    "                                output_col_name=output_col_name or \"output\")\n",
    "        print(\"[INFO] Computing average confidence across epochs\")\n",
    "        avg_confidence_across_epochs_list = self._get_average_confidence(model_weights_path=model_weights_path, batch_size=batch_size or 4)\n",
    "        print(\"[INFO] Computing variance\")\n",
    "        variability_list = self._get_variability(average_perplexity_across_epochs=avg_confidence_across_epochs_list)\n",
    "        print(\"[INFO] Computing \")\n",
    "        correctness_list = self._get_correctness(model_weights_path=model_weights_path, batch_size=batch_size or 4)\n",
    "        return avg_confidence_across_epochs_list, variability_list, correctness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/task020/all.csv\")\n",
    "df[\"input\"] = df[\"input\"].apply(lambda x: x.split(\"Now complete the following example-\\ninput: \")[-1].split(\"\\noutput: \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_difficulty = DatasetCartographyGenerativeTask(model_id=\"t5-base\", tokenizer_id=\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_difficulty.transform(input_data_or_path=df,\n",
    "                             model_weights_path=\"./output_weights\"\n",
    "                             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
