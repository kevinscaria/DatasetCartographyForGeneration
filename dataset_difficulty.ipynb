{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "message must be a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mwarnings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilterwarnings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_env/lib/python3.11/warnings.py:144\u001b[0m, in \u001b[0;36mfilterwarnings\u001b[0;34m(action, message, category, module, lineno, append)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Insert an entry into the list of warnings filters (at the front).\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m'append' -- if true, append to the list of filters\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    143\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monce\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid action: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (action,)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage must be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(category, \u001b[38;5;28mtype\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory must be a class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(category, \u001b[38;5;167;01mWarning\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory must be a Warning subclass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: message must be a string"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Union, Optional\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCartographyGenerativeTask:\n",
    "    def __init__(self, \n",
    "                 model_id: str, \n",
    "                 tokenizer_id: str,\n",
    "                 rouge_scorer_object: Optional[rouge_scorer]=None\n",
    "                 ) -> None:\n",
    "        self.model_id = model_id or \"t5-base\"\n",
    "        self.tokenizer_id = tokenizer_id or \"t5-base\"\n",
    "        self.scorer = rouge_scorer_object or rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        self.on = \"epoch\"\n",
    "        self._is_fitted = False\n",
    "\n",
    "        # These fields will be assigned by the _load_data() method\n",
    "        self.input_col_name = None\n",
    "        self.output_col_name = None\n",
    "        self.hf_data = None\n",
    "        self.input_files = None\n",
    "        self.input_labels = None\n",
    "        \n",
    "        # These fields are assigned by the _train_model() method\n",
    "        self.output_weight_path = None\n",
    "\n",
    "\n",
    "    def _load_data(self, input_file_or_path, input_col_name, output_col_name):\n",
    "        if isinstance(input_file_or_path, str):\n",
    "            df = pd.read_csv(input_file_or_path)\n",
    "        elif isinstance(input_file_or_path, pd.DataFrame):\n",
    "            df = input_file_or_path\n",
    "        self.hf_data = DatasetDict({\"train\":Dataset.from_pandas(df)})\n",
    "        self.input_col_name = input_col_name\n",
    "        self.output_col_name = output_col_name\n",
    "        try:\n",
    "            self.input_files = df[self.input_col_name].values\n",
    "        except:\n",
    "            raise Exception(f\"The input_col_name {self.input_col_name} is not a valid column name.\")\n",
    "        \n",
    "        try:\n",
    "            self.input_labels = df[self.output_col_name].values\n",
    "        except:\n",
    "            raise Exception(f\"The output_col_name {self.output_col_name} is not a valid column name.\")\n",
    "\n",
    "\n",
    "    def _train_model(self, training_arguments):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_id)\n",
    "\n",
    "        def tokenize_function(sample):\n",
    "            model_inputs = tokenizer(sample[self.input_col_name], max_length=512, truncation=True, padding=True)\n",
    "            labels = tokenizer(sample[self.output_col_name], max_length=512, truncation=True, padding=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = self.hf_data.map(tokenize_function, batched=True)\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(self.model_id)\n",
    "\n",
    "        training_args = training_arguments or \\\n",
    "            Seq2SeqTrainingArguments(output_dir=\"./output_weights\", num_train_epochs=5, save_strategy=\"epoch\", logging_strategy=\"epoch\")\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self._is_fitted = True\n",
    "        self.output_weight_path = training_args.output_dir\n",
    "\n",
    "\n",
    "    def _get_average_confidence(self, model_weights_path, batch_size):\n",
    "        ckpt_return = {}\n",
    "        for ckpt_i in tqdm(os.listdir(model_weights_path)):\n",
    "            if ckpt_i not in [\"runs\", \".DS_Store\"]:\n",
    "                ckpt_return[ckpt_i] = {}\n",
    "                model_ckpt_i = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                try:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                except:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(self.tokenizer_id)\n",
    "\n",
    "                batches = [self.input_files[i:i+batch_size] for i in range(0, len(self.input_files), batch_size)]\n",
    "                perplexity_ckpt_i = []\n",
    "\n",
    "                for batch in batches:\n",
    "                    input_batch = tokenizer_ckpt_i(batch.tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                    outputs = model_ckpt_i.generate(**input_batch, max_new_tokens=200, return_dict_in_generate=True, output_scores=True)\n",
    "                    transition_scores = model_ckpt_i.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "                    perplexity_ckpt_i.extend(transition_scores.exp().mean(dim=1).numpy().tolist())\n",
    "                ckpt_return[ckpt_i] = perplexity_ckpt_i\n",
    "        return pd.DataFrame(ckpt_return).to_numpy()\n",
    "\n",
    "\n",
    "    def _get_variability(self, average_perplexity_across_epochs):\n",
    "        return np.std(average_perplexity_across_epochs, axis=1)\n",
    "\n",
    "\n",
    "    def _get_correctness(self, model_weights_path, batch_size):\n",
    "        ckpt_return = {}\n",
    "        for ckpt_i in tqdm(os.listdir(model_weights_path)):\n",
    "            if ckpt_i not in [\"runs\", \".DS_Store\"]:\n",
    "                ckpt_return[ckpt_i] = {}\n",
    "                model_ckpt_i = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                try:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(os.path.join(model_weights_path, ckpt_i))\n",
    "                except:\n",
    "                    tokenizer_ckpt_i = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "                batches_input = [self.input_files[i:i+batch_size] for i in range(0, len(self.input_files), batch_size)]\n",
    "                batches_output = [self.input_labels[i:i+batch_size] for i in range(0, len(self.input_labels), batch_size)]\n",
    "                rouge_l_ckpt_i = []\n",
    "\n",
    "                for batch_in, batch_out in zip(batches_input, batches_output):\n",
    "                    input_batch = tokenizer_ckpt_i(batch_in.tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                    outputs = model_ckpt_i.generate(**input_batch, max_new_tokens=200, return_dict_in_generate=True)\n",
    "                    batch_gen = tokenizer_ckpt_i.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "                    for batch_out_i, batch_gen_i in zip(batch_out, batch_gen):\n",
    "                        scores = self.scorer.score(batch_out_i, batch_gen_i)\n",
    "                        score = scores[\"rougeL\"].fmeasure\n",
    "                        rouge_l_ckpt_i.append(score)\n",
    "                ckpt_return[ckpt_i] = rouge_l_ckpt_i\n",
    "        return pd.DataFrame(ckpt_return).mean(axis=1).to_numpy()\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            input_data_or_path: Union[str, pd.DataFrame],\n",
    "            input_col_name: Optional[str]=None,\n",
    "            output_col_name: Optional[str]=None,\n",
    "            training_arguments: Optional[TrainingArguments]=None\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Load dataset and assign instance variables related to data.\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        self._load_data(input_file_or_path=input_data_or_path, \n",
    "                        input_col_name=input_col_name or \"input\", \n",
    "                        output_col_name=output_col_name or \"output\")\n",
    "        self._train_model(training_arguments=training_arguments)\n",
    "\n",
    "\n",
    "    def transform(self,\n",
    "                  input_data_or_path: Optional[Union[str, pd.DataFrame]]=None,\n",
    "                  input_col_name: Optional[str]=None,\n",
    "                  output_col_name: Optional[str]=None,\n",
    "                  model_weights_path: Optional[str]=None,\n",
    "                  batch_size: Optional[int]=None\n",
    "                  ):\n",
    "        \"\"\"\n",
    "        Using the trained model checkpoints, do the following steps:\n",
    "        1. Get Average Confidence Across self.on\n",
    "        2. Get Variability\n",
    "        3. Get Correctness\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            if input_data_or_path is None or model_weights_path is None:\n",
    "                raise Exception(\"\"\"One of input_data_or_path or model_weights_path has not been assigned.\n",
    "                                Since the model has not been fit, instance fields cannot be derived.\n",
    "                                Pass values for the above mentioned arguments.\"\"\")\n",
    "            else:\n",
    "                self._load_data(input_file_or_path=input_data_or_path,\n",
    "                                input_col_name=input_col_name or \"input\",\n",
    "                                output_col_name=output_col_name or \"output\")\n",
    "        print(\"[INFO] Computing average confidence across epochs\")\n",
    "        avg_confidence_across_epochs_list = self._get_average_confidence(model_weights_path=model_weights_path, batch_size=batch_size or 4)\n",
    "        print(\"[INFO] Computing variance\")\n",
    "        variability_list = self._get_variability(average_perplexity_across_epochs=avg_confidence_across_epochs_list)\n",
    "        print(\"[INFO] Computing \")\n",
    "        correctness_list = self._get_correctness(model_weights_path=model_weights_path, batch_size=batch_size or 4)\n",
    "        return avg_confidence_across_epochs_list, variability_list, correctness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/task020/all.csv\")\n",
    "df[\"input\"] = df[\"input\"].apply(lambda x: x.split(\"Now complete the following example-\\ninput: \")[-1].split(\"\\noutput: \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_difficulty = DatasetCartographyGenerativeTask(model_id=\"t5-base\", tokenizer_id=\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing average confidence across epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3a61c4225346d99af80e2a5536bdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kscaria/miniconda3/envs/gpu_env/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing variance\n",
      "[INFO] Computing \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d080fb8c0bf4a7682cc201e9fe55b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.98162514, 0.88291025, 0.97880584, 0.9239015 , 0.96073228],\n",
       "        [0.85867673, 0.89692736, 0.8439247 , 0.90398216, 0.85133952],\n",
       "        [0.98594457, 0.90255594, 0.98201418, 0.94952208, 0.97869998],\n",
       "        ...,\n",
       "        [0.97738141, 0.94410396, 0.97707415, 0.92389172, 0.97519279],\n",
       "        [0.96209925, 0.91462475, 0.96342832, 0.88714677, 0.96460819],\n",
       "        [0.96204966, 0.87911391, 0.96226358, 0.90332127, 0.96147507]]),\n",
       " array([0.03749734, 0.02462322, 0.03135832, 0.02139524, 0.04085406,\n",
       "        0.01945448, 0.03231757, 0.02089033, 0.03627768, 0.02697046,\n",
       "        0.04697968, 0.03329914, 0.03939207, 0.03991272, 0.01746772,\n",
       "        0.02636707, 0.02948172, 0.01393115, 0.0347058 , 0.02706291,\n",
       "        0.02667771, 0.03558598, 0.03346535, 0.01781429, 0.04727125,\n",
       "        0.03318065, 0.01601809, 0.01733666, 0.02110576, 0.02725793,\n",
       "        0.03787558, 0.02112865, 0.01369312, 0.03526952, 0.03428526,\n",
       "        0.04615751, 0.02984967, 0.02673158, 0.0339194 , 0.02815728,\n",
       "        0.03996916, 0.00776955, 0.04644486, 0.03872849, 0.04342005,\n",
       "        0.02946785, 0.02474933, 0.01062233, 0.04696601, 0.01690166,\n",
       "        0.03897766, 0.03534825, 0.01337345, 0.02747422, 0.0260317 ,\n",
       "        0.042059  , 0.02576148, 0.0279493 , 0.03086316, 0.0209079 ,\n",
       "        0.03624566, 0.02238677, 0.01650834, 0.01238561, 0.02908864,\n",
       "        0.02848795, 0.03061886, 0.03381366, 0.04706722, 0.04192315,\n",
       "        0.01399228, 0.03303054, 0.03332855, 0.02344861, 0.03752316,\n",
       "        0.02019129, 0.03326658, 0.0298653 , 0.01906777, 0.03567552,\n",
       "        0.04682252, 0.03165444, 0.01118241, 0.02614992, 0.02077084,\n",
       "        0.03182936, 0.01488927, 0.02683982, 0.03528632, 0.03035324,\n",
       "        0.02617748, 0.03803494, 0.02533425, 0.02016206, 0.02265471,\n",
       "        0.02365359, 0.0323418 , 0.02181196, 0.02350536, 0.03690426,\n",
       "        0.04093356, 0.03718569, 0.02234687, 0.04524642, 0.02996545,\n",
       "        0.04038462, 0.02784306, 0.03310812, 0.04252613, 0.02068654,\n",
       "        0.03369745, 0.04104281, 0.02438076, 0.03485543, 0.04806487,\n",
       "        0.04135906, 0.04164782, 0.03456258, 0.04119893, 0.03313535,\n",
       "        0.04779618, 0.02081171, 0.02332123, 0.0232882 , 0.02311277,\n",
       "        0.02267981, 0.01772406, 0.04570622, 0.02464365, 0.0256675 ,\n",
       "        0.03176263, 0.04047029, 0.04661478, 0.03306584, 0.03497799,\n",
       "        0.01357964, 0.02824135, 0.03769017, 0.01545148, 0.03440735,\n",
       "        0.04794049, 0.03810775, 0.04483949, 0.01455744, 0.02287299,\n",
       "        0.02772893, 0.03324405, 0.03027953, 0.0229915 , 0.01113289,\n",
       "        0.03028842, 0.02525627, 0.02472063, 0.02054213, 0.04014146,\n",
       "        0.03915769, 0.03694489, 0.02384862, 0.03983001, 0.01801962,\n",
       "        0.02401727, 0.0361819 , 0.02379491, 0.04226108, 0.02027469,\n",
       "        0.0108101 , 0.02594059, 0.00978992, 0.01524894, 0.0272054 ,\n",
       "        0.04658817, 0.02800741, 0.0350362 , 0.04834531, 0.01457086,\n",
       "        0.03208655, 0.03345968, 0.05077551, 0.03728415, 0.02670802,\n",
       "        0.02683704, 0.04407695, 0.01966684, 0.01771115, 0.02217919,\n",
       "        0.02207096, 0.0226595 , 0.02032601, 0.03204732, 0.0315559 ,\n",
       "        0.03315942, 0.03872651, 0.01397897, 0.04896969, 0.04457757,\n",
       "        0.03824441, 0.0393733 , 0.02848772, 0.02974157, 0.01613299,\n",
       "        0.04131827, 0.0214979 , 0.01829886, 0.03236054, 0.03817958,\n",
       "        0.02798997, 0.02293998, 0.01659971, 0.03735307, 0.04275363,\n",
       "        0.02943868, 0.02928217, 0.0374604 , 0.03686522, 0.0387175 ,\n",
       "        0.03411315, 0.03739076, 0.03246389, 0.04816429, 0.03297557,\n",
       "        0.03628004, 0.02707448, 0.03056187, 0.03232466, 0.02632676,\n",
       "        0.03138551, 0.00724916, 0.03496143, 0.04519582, 0.02323951,\n",
       "        0.04354661, 0.0252149 , 0.04220408, 0.0493113 , 0.02582244,\n",
       "        0.03098988, 0.02685517, 0.01740279, 0.03596315, 0.03871169,\n",
       "        0.02552296, 0.04410575, 0.03393889, 0.01968934, 0.02578321,\n",
       "        0.02771753, 0.03751801, 0.01502518, 0.03992803, 0.03926765,\n",
       "        0.02996042, 0.01817512, 0.02636156, 0.0332589 , 0.02965162,\n",
       "        0.0293758 , 0.02588879, 0.03240298, 0.01698356, 0.02174314,\n",
       "        0.03463039, 0.02628198, 0.04321062, 0.02903437, 0.04381819,\n",
       "        0.01872932, 0.04826766, 0.03203853, 0.03203556, 0.04274672,\n",
       "        0.02003613, 0.04539208, 0.02393799, 0.03439305, 0.03009944,\n",
       "        0.03228706, 0.0354199 , 0.0347145 , 0.01676345, 0.04213091,\n",
       "        0.03897733, 0.03201735, 0.02780893, 0.02542111, 0.0347065 ,\n",
       "        0.04379804, 0.03491781, 0.02084874, 0.03054833, 0.02561762,\n",
       "        0.03147867, 0.02933333, 0.01882022, 0.03731094, 0.00373879,\n",
       "        0.03701157, 0.01662024, 0.02175937, 0.02850012, 0.01717272,\n",
       "        0.01376833, 0.03227121, 0.03099429, 0.02828214, 0.039172  ,\n",
       "        0.02477536, 0.02948292, 0.04823831, 0.02660873, 0.0119203 ,\n",
       "        0.02251647, 0.02992245, 0.02646884, 0.01567485, 0.04896699,\n",
       "        0.0314424 , 0.05191951, 0.04284474, 0.01219851, 0.01936006,\n",
       "        0.02645862, 0.00850002, 0.03144765, 0.02711249, 0.03033659,\n",
       "        0.02915215, 0.02822188, 0.03605431, 0.0428463 , 0.03223057,\n",
       "        0.05149318, 0.02893298, 0.03224586, 0.03478104, 0.03311569,\n",
       "        0.01817321, 0.03298558, 0.04294432, 0.01864737, 0.03340763,\n",
       "        0.03174197, 0.04602356, 0.02545291, 0.03727492, 0.03168939,\n",
       "        0.01131912, 0.03093693, 0.02840205, 0.0465211 , 0.03279339,\n",
       "        0.027462  , 0.03271271, 0.03794504, 0.04053791, 0.04083013,\n",
       "        0.03137385, 0.01331617, 0.02726566, 0.02524756, 0.03168246,\n",
       "        0.0309181 , 0.02436249, 0.04839399, 0.04128821, 0.03682169,\n",
       "        0.03167478, 0.032382  , 0.04261659, 0.02524931, 0.02435675,\n",
       "        0.02513235, 0.01265017, 0.04671823, 0.02580203, 0.02814024,\n",
       "        0.03923119, 0.02296753, 0.0183552 , 0.0218167 , 0.03183424,\n",
       "        0.03547825]),\n",
       " array([1. , 0.8, 1. , 0.2, 1. , 0. , 1. , 1. , 1. , 0.2, 1. , 0.4, 1. ,\n",
       "        0.2, 1. , 1. , 1. , 1. , 1. , 0. , 0.2, 0.8, 1. , 0.4, 1. , 1. ,\n",
       "        1. , 0.6, 1. , 0.8, 0.8, 0. , 1. , 1. , 0.8, 0.8, 1. , 0.8, 1. ,\n",
       "        0. , 0. , 0.8, 1. , 0.2, 1. , 0. , 1. , 0. , 0.6, 0.4, 1. , 0.8,\n",
       "        1. , 1. , 1. , 0.2, 0.8, 0. , 0.6, 0.6, 0.8, 0.6, 0.6, 0.4, 1. ,\n",
       "        0.2, 0. , 0.8, 0.8, 1. , 1. , 0.2, 1. , 0. , 1. , 0.2, 1. , 0. ,\n",
       "        0.6, 1. , 1. , 1. , 1. , 0. , 1. , 0.6, 1. , 0. , 1. , 0.8, 1. ,\n",
       "        1. , 1. , 0.8, 1. , 0.8, 1. , 0. , 1. , 0.4, 1. , 1. , 1. , 1. ,\n",
       "        0.6, 1. , 1. , 0. , 1. , 0.2, 1. , 0.8, 0. , 0. , 0.8, 1. , 0.2,\n",
       "        0. , 0.4, 0.8, 0.2, 1. , 1. , 0.2, 1. , 0.8, 1. , 0. , 0.8, 1. ,\n",
       "        0.6, 0. , 1. , 0.4, 1. , 0.6, 1. , 1. , 1. , 1. , 0. , 0.8, 1. ,\n",
       "        1. , 1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. , 0.6, 1. , 0.2, 0.6,\n",
       "        1. , 0. , 0.4, 0.8, 0.8, 1. , 1. , 1. , 1. , 1. , 1. , 0.4, 1. ,\n",
       "        1. , 1. , 0.2, 1. , 0.8, 1. , 0. , 1. , 1. , 0.6, 0.2, 0.8, 1. ,\n",
       "        1. , 0.6, 1. , 1. , 1. , 0.8, 0.8, 0. , 1. , 1. , 1. , 0.8, 1. ,\n",
       "        1. , 1. , 1. , 1. , 1. , 0.8, 1. , 1. , 1. , 0.6, 1. , 0. , 1. ,\n",
       "        1. , 0.6, 1. , 0.6, 1. , 1. , 1. , 0.6, 1. , 0.8, 0.8, 0.8, 1. ,\n",
       "        1. , 1. , 0. , 1. , 1. , 0.6, 1. , 0. , 0.8, 0.8, 0.8, 1. , 0.8,\n",
       "        0.2, 1. , 1. , 1. , 0.8, 1. , 0.8, 1. , 0.8, 1. , 1. , 1. , 1. ,\n",
       "        1. , 0.8, 0.6, 0.8, 1. , 1. , 1. , 0.8, 1. , 1. , 0.8, 0.4, 0.6,\n",
       "        1. , 1. , 0. , 1. , 0.4, 1. , 0.8, 0.8, 1. , 0.8, 1. , 1. , 1. ,\n",
       "        0.8, 1. , 1. , 1. , 0.6, 0.6, 1. , 1. , 1. , 1. , 1. , 0.8, 0.8,\n",
       "        1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,\n",
       "        1. , 0.6, 1. , 0. , 0.6, 0. , 1. , 1. , 1. , 0. , 0.4, 0.6, 0.6,\n",
       "        1. , 0.6, 1. , 0. , 0.6, 1. , 1. , 0.4, 0.6, 1. , 1. , 0.8, 1. ,\n",
       "        0.8, 1. , 1. , 1. , 1. , 0.4, 0.8, 1. , 0.4, 0.8, 1. , 0.8, 1. ,\n",
       "        0.6, 1. , 1. , 1. , 0.8, 1. , 0.8, 1. , 0.8, 1. , 1. , 1. , 0. ,\n",
       "        0.6, 1. , 1. , 0.2, 1. , 1. , 1. , 0.8, 1. , 0.8, 1. , 1. , 1. ,\n",
       "        1. , 0.8, 1. , 1. , 1. , 1. , 1. , 1. , 0.8, 0.8, 0.6, 1. , 1. ,\n",
       "        1. , 1. , 1. , 1. ]))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_difficulty.transform(input_data_or_path=df,\n",
    "                             model_weights_path=\"./output_weights\"\n",
    "                             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
